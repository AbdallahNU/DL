{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "* We will build a Neural Machine Translation (NMT) model to translate human-readable dates (\"25th of June, 2009\") into machine-readable dates (\"2009-06-25\"). \n",
    "* We will do this using an attention model, one of the most sophisticated sequence-to-sequence models. \n",
    "\n",
    "**Note**: this notebook was influnced by the deep learning specialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from mt_utils import *\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Input, Dense, LSTM, Activation, Lambda\n",
    "from keras.layers import Bidirectional, RepeatVector, Concatenate, Permute, Dot, multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 21091.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# define number of examples\n",
    "m = 10000\n",
    "\n",
    "# Load the data (Or actually create the data :D)\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n",
       " ('10.11.19', '2019-11-10'),\n",
       " ('9/10/70', '1970-09-10'),\n",
       " ('saturday april 28 1990', '1990-04-28'),\n",
       " ('thursday january 26 1995', '1995-01-26')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look on the data\n",
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've loaded:\n",
    "- `dataset`: a list of tuples of (human readable date, machine readable date).\n",
    "- `human_vocab`: a python dictionary mapping all characters used in the human readable dates to an integer-valued index.\n",
    "- `machine_vocab`: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. \n",
    "- `inv_machine_vocab`: the inverse dictionary of `machine_vocab`, mapping from indices back to characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "# We assume 30 is the maximum length of the human readable date.\n",
    "Tx = 30\n",
    "\n",
    "# We will set Ty=10, \"YYYY-MM-DD\" is 10 characters long.\n",
    "Ty = 10\n",
    "\n",
    "# Preprocess the data and map the raw text data into the index values\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "# Print some shapes\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have:\n",
    "- `X`: a processed version of the human readable dates in the training set.\n",
    "    - Each character in X is replaced by an index (integer) mapped to the character using `human_vocab`. \n",
    "    - Each date is padded to ensure a length of $T_x$ using a special character (< pad >). \n",
    "    - `X.shape = (m, Tx)` where m is the number of training examples in a batch.\n",
    "- `Y`: a processed version of the machine readable dates in the training set.\n",
    "    - Each character is replaced by the index (integer) it is mapped to in `machine_vocab`. \n",
    "    - `Y.shape = (m, Ty)`. \n",
    "- `Xoh`: one-hot version of `X`\n",
    "    - Each index in `X` is converted to the one-hot representation (if the index is 2, the one-hot version has the index position 2 set to 1, and the remaining positions are 0.\n",
    "    - `Xoh.shape = (m, Tx, len(human_vocab))`\n",
    "- `Yoh`: one-hot version of `Y`\n",
    "    - Each index in `Y` is converted to the one-hot representation. \n",
    "    - `Yoh.shape = (m, Ty, len(machine_vocab))`. \n",
    "    - `len(machine_vocab) = 11` since there are 10 numeric digits (0 to 9) and the `-` symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "# We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "activator = Activation(softmax, name='attention_weights')\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention\n",
    "    \n",
    "    Inputs:\n",
    "        a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "        s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "        context -- context vector, input of the next (post-attention) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s)\n",
    "    s_prev = repeator(s_prev)\n",
    "    \n",
    "    # Concatenate a and s_prev on the last axis\n",
    "    concat = concatenator([a, s_prev])\n",
    "    \n",
    "    # Use densor1 to propagate concat through a small NN\n",
    "    e = densor1(concat)\n",
    "    \n",
    "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\"\n",
    "    energies = densor2(e)\n",
    "    \n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\"\n",
    "    alphas = activator(energies)\n",
    "    \n",
    "    # Use dotor together with \"alphas\" and \"a\", to compute the context vector\n",
    "    context = dotor([alphas, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again define shared layers as global variables\n",
    "n_a = 32 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
    "n_s = 64 # number of units for the post-attention LSTM's hidden state \"s\"\n",
    "\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelf(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        Tx -- length of the input sequence\n",
    "        Ty -- length of the output sequence\n",
    "        n_a -- hidden state size of the Bi-LSTM\n",
    "        n_s -- hidden state size of the post-attention LSTM\n",
    "        human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "        machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "        model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define inputs for the model\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    \n",
    "    # Define s0 (initial hidden state) and c0 (initial cell state)\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    # Define the pre-attention Bi-LSTM.\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
    "    \n",
    "    # Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "        \n",
    "        # Perform one step of the attention mechanism to get back the context vector at step t\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "        \n",
    "        # Apply Dense layer to the hidden state output of the post-attention LSTM\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Append out to outputs list\n",
    "        outputs.append(out)\n",
    "        \n",
    "    # Create model instance taking three inputs and returning the list of outputs.\n",
    "    model = Model(inputs=[X, s0, c0], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30, 37)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 30, 64)       17920       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 30, 64)       0           s0[0][0]                         \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 lstm[8][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 30, 128)      0           bidirectional[0][0]              \n",
      "                                                                 repeat_vector[0][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[1][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[2][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[3][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[4][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[5][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[6][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[7][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[8][0]              \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 repeat_vector[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 30, 10)       1290        concatenate[0][0]                \n",
      "                                                                 concatenate[1][0]                \n",
      "                                                                 concatenate[2][0]                \n",
      "                                                                 concatenate[3][0]                \n",
      "                                                                 concatenate[4][0]                \n",
      "                                                                 concatenate[5][0]                \n",
      "                                                                 concatenate[6][0]                \n",
      "                                                                 concatenate[7][0]                \n",
      "                                                                 concatenate[8][0]                \n",
      "                                                                 concatenate[9][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30, 1)        11          dense[0][0]                      \n",
      "                                                                 dense[1][0]                      \n",
      "                                                                 dense[2][0]                      \n",
      "                                                                 dense[3][0]                      \n",
      "                                                                 dense[4][0]                      \n",
      "                                                                 dense[5][0]                      \n",
      "                                                                 dense[6][0]                      \n",
      "                                                                 dense[7][0]                      \n",
      "                                                                 dense[8][0]                      \n",
      "                                                                 dense[9][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 64)        0           attention_weights[0][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 64), (None,  33024       dot[0][0]                        \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot[1][0]                        \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "                                                                 dot[2][0]                        \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 lstm[1][2]                       \n",
      "                                                                 dot[3][0]                        \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 lstm[2][2]                       \n",
      "                                                                 dot[4][0]                        \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 lstm[3][2]                       \n",
      "                                                                 dot[5][0]                        \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 lstm[4][2]                       \n",
      "                                                                 dot[6][0]                        \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 lstm[5][2]                       \n",
      "                                                                 dot[7][0]                        \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 lstm[6][2]                       \n",
      "                                                                 dot[8][0]                        \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 lstm[7][2]                       \n",
      "                                                                 dot[9][0]                        \n",
      "                                                                 lstm[8][0]                       \n",
      "                                                                 lstm[8][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 11)           715         lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 lstm[8][0]                       \n",
      "                                                                 lstm[9][0]                       \n",
      "==================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = modelf(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = Adam(lr=0.005, decay=0.01)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "79/79 [==============================] - 20s 62ms/step - loss: 20.7285 - dense_2_loss: 1.9194 - dense_2_1_loss: 1.7789 - dense_2_2_loss: 2.2249 - dense_2_3_loss: 2.7320 - dense_2_4_loss: 1.4360 - dense_2_5_loss: 1.7421 - dense_2_6_loss: 2.7472 - dense_2_7_loss: 1.4584 - dense_2_8_loss: 1.9633 - dense_2_9_loss: 2.7265 - dense_2_accuracy: 0.1910 - dense_2_1_accuracy: 0.4331 - dense_2_2_accuracy: 0.1846 - dense_2_3_accuracy: 0.0843 - dense_2_4_accuracy: 0.7742 - dense_2_5_accuracy: 0.1022 - dense_2_6_accuracy: 0.0126 - dense_2_7_accuracy: 0.8095 - dense_2_8_accuracy: 0.1314 - dense_2_9_accuracy: 0.0688\n",
      "Epoch 2/25\n",
      "79/79 [==============================] - 5s 63ms/step - loss: 11.9721 - dense_2_loss: 0.3819 - dense_2_1_loss: 0.3161 - dense_2_2_loss: 1.2658 - dense_2_3_loss: 2.4564 - dense_2_4_loss: 0.2483 - dense_2_5_loss: 0.7699 - dense_2_6_loss: 2.5404 - dense_2_7_loss: 0.2797 - dense_2_8_loss: 1.4075 - dense_2_9_loss: 2.3062 - dense_2_accuracy: 0.9136 - dense_2_1_accuracy: 0.9218 - dense_2_2_accuracy: 0.4869 - dense_2_3_accuracy: 0.1211 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.7599 - dense_2_6_accuracy: 0.1284 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.4056 - dense_2_9_accuracy: 0.1442\n",
      "Epoch 3/25\n",
      "79/79 [==============================] - 5s 64ms/step - loss: 8.8617 - dense_2_loss: 0.1528 - dense_2_1_loss: 0.1219 - dense_2_2_loss: 0.9003 - dense_2_3_loss: 2.1516 - dense_2_4_loss: 0.0266 - dense_2_5_loss: 0.3472 - dense_2_6_loss: 1.8556 - dense_2_7_loss: 0.0211 - dense_2_8_loss: 1.1307 - dense_2_9_loss: 2.1538 - dense_2_accuracy: 0.9645 - dense_2_1_accuracy: 0.9659 - dense_2_2_accuracy: 0.6677 - dense_2_3_accuracy: 0.2158 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.8968 - dense_2_6_accuracy: 0.3300 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.5086 - dense_2_9_accuracy: 0.2060\n",
      "Epoch 4/25\n",
      "79/79 [==============================] - 5s 64ms/step - loss: 7.7184 - dense_2_loss: 0.1113 - dense_2_1_loss: 0.0950 - dense_2_2_loss: 0.6987 - dense_2_3_loss: 1.9163 - dense_2_4_loss: 0.0195 - dense_2_5_loss: 0.2054 - dense_2_6_loss: 1.6204 - dense_2_7_loss: 0.0149 - dense_2_8_loss: 0.9953 - dense_2_9_loss: 2.0416 - dense_2_accuracy: 0.9724 - dense_2_1_accuracy: 0.9737 - dense_2_2_accuracy: 0.7672 - dense_2_3_accuracy: 0.3107 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9483 - dense_2_6_accuracy: 0.4195 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.5821 - dense_2_9_accuracy: 0.2520\n",
      "Epoch 5/25\n",
      "79/79 [==============================] - 5s 67ms/step - loss: 6.8406 - dense_2_loss: 0.0898 - dense_2_1_loss: 0.0772 - dense_2_2_loss: 0.5524 - dense_2_3_loss: 1.6609 - dense_2_4_loss: 0.0197 - dense_2_5_loss: 0.1675 - dense_2_6_loss: 1.4711 - dense_2_7_loss: 0.0147 - dense_2_8_loss: 0.8902 - dense_2_9_loss: 1.8970 - dense_2_accuracy: 0.9770 - dense_2_1_accuracy: 0.9794 - dense_2_2_accuracy: 0.8187 - dense_2_3_accuracy: 0.4104 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9536 - dense_2_6_accuracy: 0.4683 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.6327 - dense_2_9_accuracy: 0.3076\n",
      "Epoch 6/25\n",
      "79/79 [==============================] - 5s 67ms/step - loss: 6.0622 - dense_2_loss: 0.0787 - dense_2_1_loss: 0.0682 - dense_2_2_loss: 0.4797 - dense_2_3_loss: 1.4229 - dense_2_4_loss: 0.0163 - dense_2_5_loss: 0.1444 - dense_2_6_loss: 1.3477 - dense_2_7_loss: 0.0122 - dense_2_8_loss: 0.7830 - dense_2_9_loss: 1.7090 - dense_2_accuracy: 0.9788 - dense_2_1_accuracy: 0.9813 - dense_2_2_accuracy: 0.8399 - dense_2_3_accuracy: 0.5015 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9608 - dense_2_6_accuracy: 0.5152 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.6816 - dense_2_9_accuracy: 0.3822\n",
      "Epoch 7/25\n",
      "79/79 [==============================] - 5s 66ms/step - loss: 5.3328 - dense_2_loss: 0.0718 - dense_2_1_loss: 0.0621 - dense_2_2_loss: 0.4322 - dense_2_3_loss: 1.1900 - dense_2_4_loss: 0.0145 - dense_2_5_loss: 0.1315 - dense_2_6_loss: 1.2266 - dense_2_7_loss: 0.0113 - dense_2_8_loss: 0.7093 - dense_2_9_loss: 1.4837 - dense_2_accuracy: 0.9811 - dense_2_1_accuracy: 0.9813 - dense_2_2_accuracy: 0.8535 - dense_2_3_accuracy: 0.6039 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9644 - dense_2_6_accuracy: 0.5668 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.7247 - dense_2_9_accuracy: 0.4577\n",
      "Epoch 8/25\n",
      "79/79 [==============================] - 5s 69ms/step - loss: 4.6540 - dense_2_loss: 0.0690 - dense_2_1_loss: 0.0609 - dense_2_2_loss: 0.3716 - dense_2_3_loss: 0.9769 - dense_2_4_loss: 0.0138 - dense_2_5_loss: 0.1159 - dense_2_6_loss: 1.0795 - dense_2_7_loss: 0.0101 - dense_2_8_loss: 0.6539 - dense_2_9_loss: 1.3023 - dense_2_accuracy: 0.9821 - dense_2_1_accuracy: 0.9811 - dense_2_2_accuracy: 0.8791 - dense_2_3_accuracy: 0.6846 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9702 - dense_2_6_accuracy: 0.6438 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.7497 - dense_2_9_accuracy: 0.5330\n",
      "Epoch 9/25\n",
      "79/79 [==============================] - 5s 65ms/step - loss: 4.0366 - dense_2_loss: 0.0614 - dense_2_1_loss: 0.0528 - dense_2_2_loss: 0.3496 - dense_2_3_loss: 0.8110 - dense_2_4_loss: 0.0126 - dense_2_5_loss: 0.1079 - dense_2_6_loss: 0.9274 - dense_2_7_loss: 0.0096 - dense_2_8_loss: 0.6048 - dense_2_9_loss: 1.0996 - dense_2_accuracy: 0.9830 - dense_2_1_accuracy: 0.9821 - dense_2_2_accuracy: 0.8876 - dense_2_3_accuracy: 0.7406 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9732 - dense_2_6_accuracy: 0.7153 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.7822 - dense_2_9_accuracy: 0.6146\n",
      "Epoch 10/25\n",
      "79/79 [==============================] - 5s 66ms/step - loss: 3.5540 - dense_2_loss: 0.0572 - dense_2_1_loss: 0.0475 - dense_2_2_loss: 0.3160 - dense_2_3_loss: 0.6962 - dense_2_4_loss: 0.0107 - dense_2_5_loss: 0.0967 - dense_2_6_loss: 0.8121 - dense_2_7_loss: 0.0094 - dense_2_8_loss: 0.5765 - dense_2_9_loss: 0.9318 - dense_2_accuracy: 0.9842 - dense_2_1_accuracy: 0.9864 - dense_2_2_accuracy: 0.9024 - dense_2_3_accuracy: 0.7923 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9775 - dense_2_6_accuracy: 0.7651 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.7891 - dense_2_9_accuracy: 0.6932\n",
      "Epoch 11/25\n",
      "79/79 [==============================] - 5s 67ms/step - loss: 3.1898 - dense_2_loss: 0.0542 - dense_2_1_loss: 0.0444 - dense_2_2_loss: 0.2959 - dense_2_3_loss: 0.5941 - dense_2_4_loss: 0.0098 - dense_2_5_loss: 0.0939 - dense_2_6_loss: 0.7408 - dense_2_7_loss: 0.0091 - dense_2_8_loss: 0.5427 - dense_2_9_loss: 0.8050 - dense_2_accuracy: 0.9837 - dense_2_1_accuracy: 0.9864 - dense_2_2_accuracy: 0.8990 - dense_2_3_accuracy: 0.8196 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9764 - dense_2_6_accuracy: 0.7859 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.8045 - dense_2_9_accuracy: 0.7390\n",
      "Epoch 12/25\n",
      "79/79 [==============================] - 5s 64ms/step - loss: 2.8803 - dense_2_loss: 0.0542 - dense_2_1_loss: 0.0464 - dense_2_2_loss: 0.2839 - dense_2_3_loss: 0.5331 - dense_2_4_loss: 0.0090 - dense_2_5_loss: 0.0849 - dense_2_6_loss: 0.6488 - dense_2_7_loss: 0.0090 - dense_2_8_loss: 0.5074 - dense_2_9_loss: 0.7036 - dense_2_accuracy: 0.9840 - dense_2_1_accuracy: 0.9855 - dense_2_2_accuracy: 0.9021 - dense_2_3_accuracy: 0.8387 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9795 - dense_2_6_accuracy: 0.8183 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.8154 - dense_2_9_accuracy: 0.7687\n",
      "Epoch 13/25\n",
      "79/79 [==============================] - 5s 66ms/step - loss: 2.6010 - dense_2_loss: 0.0471 - dense_2_1_loss: 0.0370 - dense_2_2_loss: 0.2599 - dense_2_3_loss: 0.4732 - dense_2_4_loss: 0.0083 - dense_2_5_loss: 0.0814 - dense_2_6_loss: 0.5949 - dense_2_7_loss: 0.0089 - dense_2_8_loss: 0.4735 - dense_2_9_loss: 0.6168 - dense_2_accuracy: 0.9872 - dense_2_1_accuracy: 0.9889 - dense_2_2_accuracy: 0.9100 - dense_2_3_accuracy: 0.8429 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9766 - dense_2_6_accuracy: 0.8337 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.8278 - dense_2_9_accuracy: 0.8032\n",
      "Epoch 14/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 5s 61ms/step - loss: 2.3888 - dense_2_loss: 0.0453 - dense_2_1_loss: 0.0349 - dense_2_2_loss: 0.2443 - dense_2_3_loss: 0.4205 - dense_2_4_loss: 0.0072 - dense_2_5_loss: 0.0791 - dense_2_6_loss: 0.5404 - dense_2_7_loss: 0.0082 - dense_2_8_loss: 0.4514 - dense_2_9_loss: 0.5575 - dense_2_accuracy: 0.9862 - dense_2_1_accuracy: 0.9890 - dense_2_2_accuracy: 0.9132 - dense_2_3_accuracy: 0.8646 - dense_2_4_accuracy: 0.9999 - dense_2_5_accuracy: 0.9783 - dense_2_6_accuracy: 0.8556 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.8393 - dense_2_9_accuracy: 0.8216\n",
      "Epoch 15/25\n",
      "79/79 [==============================] - 5s 61ms/step - loss: 2.1491 - dense_2_loss: 0.0413 - dense_2_1_loss: 0.0309 - dense_2_2_loss: 0.2199 - dense_2_3_loss: 0.3753 - dense_2_4_loss: 0.0070 - dense_2_5_loss: 0.0740 - dense_2_6_loss: 0.4927 - dense_2_7_loss: 0.0073 - dense_2_8_loss: 0.4123 - dense_2_9_loss: 0.4885 - dense_2_accuracy: 0.9882 - dense_2_1_accuracy: 0.9903 - dense_2_2_accuracy: 0.9235 - dense_2_3_accuracy: 0.8752 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9797 - dense_2_6_accuracy: 0.8648 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.8524 - dense_2_9_accuracy: 0.8456\n",
      "Epoch 16/25\n",
      "79/79 [==============================] - 5s 59ms/step - loss: 2.0098 - dense_2_loss: 0.0392 - dense_2_1_loss: 0.0296 - dense_2_2_loss: 0.2076 - dense_2_3_loss: 0.3515 - dense_2_4_loss: 0.0062 - dense_2_5_loss: 0.0725 - dense_2_6_loss: 0.4460 - dense_2_7_loss: 0.0071 - dense_2_8_loss: 0.3999 - dense_2_9_loss: 0.4502 - dense_2_accuracy: 0.9873 - dense_2_1_accuracy: 0.9913 - dense_2_2_accuracy: 0.9278 - dense_2_3_accuracy: 0.8764 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9808 - dense_2_6_accuracy: 0.8822 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.8552 - dense_2_9_accuracy: 0.8592\n",
      "Epoch 17/25\n",
      "79/79 [==============================] - 5s 59ms/step - loss: 1.8944 - dense_2_loss: 0.0421 - dense_2_1_loss: 0.0318 - dense_2_2_loss: 0.2018 - dense_2_3_loss: 0.3292 - dense_2_4_loss: 0.0056 - dense_2_5_loss: 0.0692 - dense_2_6_loss: 0.4354 - dense_2_7_loss: 0.0066 - dense_2_8_loss: 0.3700 - dense_2_9_loss: 0.4028 - dense_2_accuracy: 0.9866 - dense_2_1_accuracy: 0.9901 - dense_2_2_accuracy: 0.9293 - dense_2_3_accuracy: 0.8837 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9808 - dense_2_6_accuracy: 0.8817 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.8657 - dense_2_9_accuracy: 0.8703\n",
      "Epoch 18/25\n",
      "79/79 [==============================] - 5s 59ms/step - loss: 1.7539 - dense_2_loss: 0.0402 - dense_2_1_loss: 0.0309 - dense_2_2_loss: 0.1936 - dense_2_3_loss: 0.3044 - dense_2_4_loss: 0.0051 - dense_2_5_loss: 0.0652 - dense_2_6_loss: 0.4017 - dense_2_7_loss: 0.0068 - dense_2_8_loss: 0.3384 - dense_2_9_loss: 0.3677 - dense_2_accuracy: 0.9861 - dense_2_1_accuracy: 0.9894 - dense_2_2_accuracy: 0.9260 - dense_2_3_accuracy: 0.8972 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9825 - dense_2_6_accuracy: 0.8899 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.8795 - dense_2_9_accuracy: 0.8841\n",
      "Epoch 19/25\n",
      "79/79 [==============================] - 5s 59ms/step - loss: 1.6176 - dense_2_loss: 0.0351 - dense_2_1_loss: 0.0254 - dense_2_2_loss: 0.1821 - dense_2_3_loss: 0.2789 - dense_2_4_loss: 0.0046 - dense_2_5_loss: 0.0615 - dense_2_6_loss: 0.3766 - dense_2_7_loss: 0.0064 - dense_2_8_loss: 0.3105 - dense_2_9_loss: 0.3366 - dense_2_accuracy: 0.9879 - dense_2_1_accuracy: 0.9916 - dense_2_2_accuracy: 0.9322 - dense_2_3_accuracy: 0.9105 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9833 - dense_2_6_accuracy: 0.9012 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.8891 - dense_2_9_accuracy: 0.9029\n",
      "Epoch 20/25\n",
      "79/79 [==============================] - 5s 58ms/step - loss: 1.5648 - dense_2_loss: 0.0361 - dense_2_1_loss: 0.0271 - dense_2_2_loss: 0.1776 - dense_2_3_loss: 0.2605 - dense_2_4_loss: 0.0046 - dense_2_5_loss: 0.0614 - dense_2_6_loss: 0.3679 - dense_2_7_loss: 0.0057 - dense_2_8_loss: 0.3054 - dense_2_9_loss: 0.3185 - dense_2_accuracy: 0.9878 - dense_2_1_accuracy: 0.9917 - dense_2_2_accuracy: 0.9359 - dense_2_3_accuracy: 0.9206 - dense_2_4_accuracy: 0.9997 - dense_2_5_accuracy: 0.9823 - dense_2_6_accuracy: 0.8998 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.8926 - dense_2_9_accuracy: 0.9036\n",
      "Epoch 21/25\n",
      "79/79 [==============================] - 5s 59ms/step - loss: 1.4566 - dense_2_loss: 0.0316 - dense_2_1_loss: 0.0228 - dense_2_2_loss: 0.1644 - dense_2_3_loss: 0.2463 - dense_2_4_loss: 0.0043 - dense_2_5_loss: 0.0605 - dense_2_6_loss: 0.3496 - dense_2_7_loss: 0.0056 - dense_2_8_loss: 0.2799 - dense_2_9_loss: 0.2917 - dense_2_accuracy: 0.9898 - dense_2_1_accuracy: 0.9924 - dense_2_2_accuracy: 0.9402 - dense_2_3_accuracy: 0.9246 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9826 - dense_2_6_accuracy: 0.9086 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9020 - dense_2_9_accuracy: 0.9100\n",
      "Epoch 22/25\n",
      "79/79 [==============================] - 5s 58ms/step - loss: 1.3537 - dense_2_loss: 0.0303 - dense_2_1_loss: 0.0222 - dense_2_2_loss: 0.1481 - dense_2_3_loss: 0.2216 - dense_2_4_loss: 0.0040 - dense_2_5_loss: 0.0525 - dense_2_6_loss: 0.3344 - dense_2_7_loss: 0.0054 - dense_2_8_loss: 0.2594 - dense_2_9_loss: 0.2757 - dense_2_accuracy: 0.9907 - dense_2_1_accuracy: 0.9934 - dense_2_2_accuracy: 0.9484 - dense_2_3_accuracy: 0.9383 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9875 - dense_2_6_accuracy: 0.9130 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9085 - dense_2_9_accuracy: 0.9138\n",
      "Epoch 23/25\n",
      "79/79 [==============================] - 5s 59ms/step - loss: 1.2860 - dense_2_loss: 0.0308 - dense_2_1_loss: 0.0201 - dense_2_2_loss: 0.1453 - dense_2_3_loss: 0.2182 - dense_2_4_loss: 0.0037 - dense_2_5_loss: 0.0516 - dense_2_6_loss: 0.3184 - dense_2_7_loss: 0.0051 - dense_2_8_loss: 0.2442 - dense_2_9_loss: 0.2486 - dense_2_accuracy: 0.9886 - dense_2_1_accuracy: 0.9936 - dense_2_2_accuracy: 0.9489 - dense_2_3_accuracy: 0.9412 - dense_2_4_accuracy: 0.9998 - dense_2_5_accuracy: 0.9844 - dense_2_6_accuracy: 0.9177 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9143 - dense_2_9_accuracy: 0.9270\n",
      "Epoch 24/25\n",
      "79/79 [==============================] - 5s 59ms/step - loss: 1.2051 - dense_2_loss: 0.0271 - dense_2_1_loss: 0.0187 - dense_2_2_loss: 0.1370 - dense_2_3_loss: 0.1963 - dense_2_4_loss: 0.0039 - dense_2_5_loss: 0.0488 - dense_2_6_loss: 0.2993 - dense_2_7_loss: 0.0046 - dense_2_8_loss: 0.2310 - dense_2_9_loss: 0.2384 - dense_2_accuracy: 0.9924 - dense_2_1_accuracy: 0.9941 - dense_2_2_accuracy: 0.9570 - dense_2_3_accuracy: 0.9521 - dense_2_4_accuracy: 0.9998 - dense_2_5_accuracy: 0.9855 - dense_2_6_accuracy: 0.9207 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9211 - dense_2_9_accuracy: 0.9288\n",
      "Epoch 25/25\n",
      "79/79 [==============================] - 5s 58ms/step - loss: 1.1533 - dense_2_loss: 0.0265 - dense_2_1_loss: 0.0186 - dense_2_2_loss: 0.1263 - dense_2_3_loss: 0.1958 - dense_2_4_loss: 0.0036 - dense_2_5_loss: 0.0461 - dense_2_6_loss: 0.2879 - dense_2_7_loss: 0.0043 - dense_2_8_loss: 0.2197 - dense_2_9_loss: 0.2245 - dense_2_accuracy: 0.9916 - dense_2_1_accuracy: 0.9938 - dense_2_2_accuracy: 0.9612 - dense_2_3_accuracy: 0.9479 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9867 - dense_2_6_accuracy: 0.9262 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9282 - dense_2_9_accuracy: 0.9324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f720435ee50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=25, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(model, filepath='my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-03 \n",
      "\n",
      "source: 5 April 09\n",
      "output: 2009-04-04 \n",
      "\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-22 \n",
      "\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10 \n",
      "\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09 \n",
      "\n",
      "source: March 3 2001\n",
      "output: 2001-03-03 \n",
      "\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03 \n",
      "\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "s00 = np.zeros((1, n_s))\n",
    "c00 = np.zeros((1, n_s))\n",
    "for example in EXAMPLES:\n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    #print(source)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "    source = np.swapaxes(source, 0, 1)\n",
    "    source = np.expand_dims(source, axis=0)\n",
    "    prediction = model.predict([source, s00, c00])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
